#!/usr/bin/env python3

################################################################################
# download entire current PDB and process to extract structural information
#
# (C) 2020 Bryan Kolaczkowski, University of Florida, Gainesville, FL USA
# Released under GNU General Public License (GPL)
# bryank@ufl.edu
################################################################################

# he he :) - get path to this running file to import the prot3d module
import os
libpath = os.path.normpath(                                                   \
            os.path.join(                                                     \
                os.path.dirname(os.path.abspath(os.path.realpath(__file__))), \
                '..')                                                         \
            )
import sys
sys.path.append(libpath)

import multiprocessing
import distutils.util
import functools
import tempfile
import argparse
import warnings
import traceback
import time
import glob
import csv
import io
import re
import bz2

import numpy
import requests
import Bio.PDB
import Bio.PDB.Polypeptide

from prot3d._version import __version__

################################################################################
# BEG DEFINE DEFAULTS

PDB_QUERY_URL = 'https://www.rcsb.org/pdb/rest/customReport'
PDB_SFILE_URL = 'https://files.rcsb.org/download'
SHORTEST_POLYPEPTIDE = 20   # shortest allowable polypeptide
ROUND = 4   # number of decimal places to round numbers to

# END DEFINE DEFAULTS
################################################################################
################################################################################
# BEG CLASS DEFINITIONS

class DownloadException(Exception):
  pass

class StructureException(Exception):
  pass

class DSSPException(Exception):
  pass

class FixDisordered(Bio.PDB.Select):
  def accept_atom(self, atom):
    # keep only disordered atoms with the 'A' altloc identifier
    if (not atom.is_disordered()) or atom.get_altloc() == 'A':
      atom.set_altloc(' ')  # Eliminate alt location ID before output.
      return True
    else:
      return False

# END CLASS DEFINITIONS
################################################################################
################################################################################
# BEG HELPER FUNCTIONS

def _fix_url(url):
    """removes trailing / from URL"""
    if url[-1] == '/':
        return url[:-1]
    return url

def _get_pdb_ids(request_url):
    """
    Returns a list of PDB IDs at the request_url
    structures must have at least 1 protein
    """
    mol_type_wanted = r'Protein'

    fields = ['structureId',
              'macromoleculeType',
              'experimentalTechnique',
              'releaseDate',
              'resolution']

    idwldcrd = '?pdbids=*'
    crepcols = '&customReportColumns={}'.format(','.join(fields))
    otformat = '&service=wsfile&format=csv'

    request = request_url + idwldcrd + crepcols + otformat
    result = requests.get(request)
    result.raise_for_status()

    instream = io.StringIO(result.text, newline='')
    reader = csv.reader(instream)
    reader.__next__() # skip csv header
    filtered_ids = []
    for row in reader:
        if re.search(mol_type_wanted, row[1]):
            filtered_ids.append(row[0].lower())
    filtered_ids.sort()
    return filtered_ids

def _get_pdb_text(base_url, pdbid, fname_extension):
  """
  Returns the structural text for the given pdbid and base_url
  fname_extension should be 'cif' or 'pdb'
  raises DownloadException on error
  """
  outfname = 'structure.{}'.format(fname_extension)
  full_url  = '{}/{}.{}'.format(base_url, pdbid.upper(), fname_extension)
  max_tries = 100
  tries     = 0
  while tries < max_tries:
      try:
          tries += 1
          # get structure from full_url
          result = requests.get(full_url, timeout=10)
          # check result is okay
          result.raise_for_status()
          # return text for structure
          return result.text
      except:
          pass
  raise DownloadException(pdbid)

def _get_model(base_url, pdbid, fname_extension):
  # set up parser based on fname_extension
  if fname_extension == 'cif':
    reader = Bio.PDB.MMCIFParser(QUIET=True)
  elif fname_extension == 'pdb':
    reader = Bio.PDB.PDBParser(QUIET=True)
  else:
    return None
  # write structure text into temporary file and parse it
  tmpstrfname = 'structure_download_temp.txt'
  with open(tmpstrfname, 'w') as outf:
    outf.write(_get_pdb_text(base_url, pdbid, fname_extension))
  structure = reader.get_structure('sid', tmpstrfname)
  # return first model in the structure
  return structure[0]

def _get_polypeptides(base_url, pdbid):
  """
  Returns a list of polypeptides for the given pdbid and base_url
  """
  # try cif reader first
  try:
    str_model = _get_model(base_url, pdbid, 'cif')
  except:
    # try pdb reader if cif reader fails
    str_model = _get_model(base_url, pdbid, 'pdb')
  ppb = Bio.PDB.PPBuilder()
  return ppb.build_peptides(str_model)

def _get_dssp(model, fname):
  results = []
  with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    dssp = Bio.PDB.DSSP(model, fname, acc_array='Wilke')
    for k in dssp.keys():
      residu = dssp[k][1].upper()  # residue 1-letter code
      secstr = dssp[k][2].upper()  # secondary struc code
      # hack to fix 'NA' entries for rasa values in dssp
      # assume (rare) missing and non-standard amino acids
      # are burried?
      if dssp[k][3] == 'NA':
        relasa = 0.0
      else:
        relasa = round(float(dssp[k][3]), ROUND)
      phiang = round(float(dssp[k][4]), ROUND)
      psiang = round(float(dssp[k][5]), ROUND)
      results.append([residu, secstr, relasa, phiang, psiang])
  return results

"""
def _get_pairwise_dists(aa_residues):
  dst_mat = []
  for i in range(len(aa_residues)):
    crd1  = aa_residues[i]
    dst_arr = []
    for j in range(i):
      crd2 = aa_residues[j]
      dst = round(numpy.linalg.norm(numpy.array(crd1)-numpy.array(crd2)), ROUND)
      dst_arr.append(dst)
    dst_mat.append(dst_arr)
  return dst_mat

def _get_dist(fname):
  reader    = Bio.PDB.PDBParser(QUIET=True)
  structure = reader.get_structure('sid', fname)
  model     = structure[0]
  # get location of each Cb (or Ca for GLY) atom,
  # for each residue in the structural model
  # assumes single-model, single-chain, single-peptide structure
  res_locs  = []
  for res in model.get_residues():
    try:
      ca_loc = res['CB'].get_coord()
    except:
      ca_loc = res['CA'].get_coord()
    res_locs.append(ca_loc)
  return _get_pairwise_dists(res_locs)
"""

def _process_pdb(base_url, pdbid):
  results = []
  # get polypeptides for this structure
  polypeptides = _get_polypeptides(base_url, pdbid)
  if len(polypeptides) < 1:
    raise StructureException('ERRR: {} has no polypeptides'.format(pdbid))
  # setup for writing each polypeptide as its own pdb file
  pdb_writer = Bio.PDB.PDBIO()
  model_id = 1
  for polyp in polypeptides:
    if len(polyp) < SHORTEST_POLYPEPTIDE:
      continue
    try:
      # build chain
      my_chain = Bio.PDB.Chain.Chain('A')
      for r in polyp:
        my_chain.add(r)
      # build model
      my_model = Bio.PDB.Model.Model(model_id)
      my_model.add(my_chain)
      # build structure
      my_structure = Bio.PDB.Structure.Structure(pdbid)
      my_structure.add(my_model)
      # write pdb file
      pdb_fname = '{}_{}.pdb'.format(pdbid, model_id)
      pdb_writer.set_structure(my_structure)
      pdb_writer.save(pdb_fname, select=FixDisordered())
      # get dssp results
      dssp_results = _get_dssp(my_model, pdb_fname)
      # get residue-residue distances
      #dist_results = _get_dist(pdb_fname)
      # append to results
      #if len(dssp_results) != len(dist_results):
      #  raise StructureException('dssp and distances have different sizes')
      results.append((pdbid, model_id, dssp_results))
      model_id += 1
    except Exception as e:
      sys.stderr.write('ERRR: {} {} failed\n'.format(pdbid, model_id))
      traceback.print_exc()
  return results

def runPDB(outdir, url_base, pdbid):
  workingdir = os.getcwd()
  with tempfile.TemporaryDirectory(prefix='getPDBdata-{}-'.format(pdbid)) \
                                   as tempdir:
    os.chdir(tempdir)
    try:
      for id1,id2,dssps in _process_pdb(url_base, pdbid):
        with bz2.open(outdir + os.path.sep + '{}_{}.csv'.format(id1,id2),
                      mode='wb', compresslevel=9) as outf:
          writer = csv.writer(outf, dialect='unix', quoting=csv.QUOTE_MINIMAL)
          writer.writerow(['resi',
                           'sstr',
                           'rasa',
                           'phi',
                           'psi'])
          for row in dssps:
            writer.writerow(row)
    except Exception as e:
      traceback.print_exc()
    finally:
      os.chdir(workingdir)

# END HELPER FUNCTIONS
################################################################################
################################################################################
# BEG MAIN

if __name__ == '__main__':
    # parse command-line arguments
    parser = argparse.ArgumentParser(
                      description='infers structural information across PDB',
                      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    # general options
    parser.add_argument('--version', action='version', version=__version__)
    parser.add_argument('-v', '--verbose', type=distutils.util.strtobool,
                        dest='verbose',
                        help='show runtime information on stdout',
                        metavar='y|n')
    parser.add_argument('-t', '--threads', type=int, dest='threads',
                        help='set number of execution threads (0 to use all)',
                        metavar='N')
    parser.add_argument('-o', '--outdir', dest='outdir',
                        help='set output directory',
                        metavar='DIR', required=True)
    parser.add_argument('--pdbqueryurl', dest='pdb_query_url',
                        help='set the URL for PDB queries',
                        metavar='URL')
    parser.add_argument('--pdbfileurl', dest='pdb_sfile_url',
                        help='set the URL for PDB file download',
                        metavar='URL')
    parser.add_argument('--exclude', dest='pdb_exclude_f',
                        help='set filename for pdb ids to exclude',
                        metavar='FILE')
    # set defaults
    parser.set_defaults(verbose=True,
                        threads=0,
                        outdir=None,
                        pdb_query_url=PDB_QUERY_URL,
                        pdb_sfile_url=PDB_SFILE_URL,
                        pdb_exclude_f=None)
    # parse command-line arguments
    args = parser.parse_args()
    if args.threads <= 0:
        args.threads = multiprocessing.cpu_count()
    args.pdb_query_url = _fix_url(args.pdb_query_url)
    args.pdb_sfile_url = _fix_url(args.pdb_sfile_url)
    args.outdir        = os.path.abspath(_fix_url(args.outdir))

    # get list of all PDB ids
    pdb_ids = _get_pdb_ids(args.pdb_query_url)
    if args.verbose:
        print('found {} IDs in current PDB'.format(len(pdb_ids)))

    # set up output directory
    if not os.path.isdir(args.outdir):
        os.makedirs(args.outdir)

    # filter list of pdb_ids by ids already processed
    done_pdb_ids = set([])
    for f in glob.iglob(args.outdir + os.path.sep + '*_*.csv'):
        done_pdb_ids.add(f.split(os.path.sep)[-1].split('_')[0])
    filtered_pdb_ids = list(set(pdb_ids) - done_pdb_ids)

    # optionally, filter list of pdb_ids by exclude file
    if args.pdb_exclude_f:
        exclude_pdb_ids = set([])
        with open(args.pdb_exclude_f, 'r') as handle:
            for line in handle:
                exclude_pdb_ids.add(line.strip())
        filtered_pdb_ids = list(set(filtered_pdb_ids) - exclude_pdb_ids)
        if args.verbose:
            print('excluding {}'.format(len(exclude_pdb_ids)))

    filtered_pdb_ids.sort()

    if args.verbose:
        print('have {} completed; need {}'.format(len(done_pdb_ids),
                                                  len(filtered_pdb_ids)))

    # write structural information files
    with multiprocessing.Pool(args.threads) as threadpool:
        targetfn = functools.partial(runPDB, args.outdir, args.pdb_sfile_url)
        threadpool.map(targetfn, filtered_pdb_ids,
                       chunksize=min(1,len(filtered_pdb_ids)//args.threads))

    if args.verbose:
        print('finished.')

# END MAIN
################################################################################
