#!/usr/bin/env python3

################################################################################
# splits pdb data into training, validation and testing data sets
#
# (C) 2020 Bryan Kolaczkowski, University of Florida, Gainesville, FL USA
# Released under GNU General Public License (GPL)
# bryank@ufl.edu
################################################################################

# he he :) - get path to this running file to import the prot3d module
import os
libpath = os.path.normpath(                                                   \
            os.path.join(                                                     \
                os.path.dirname(os.path.abspath(os.path.realpath(__file__))), \
                '..')                                                         \
            )
import sys
sys.path.append(libpath)

import distutils.util
import argparse
import math
import csv
import numpy

from prot3d._version import __version__

################################################################################
# BEG DEFINE DEFAULTS

DEF_RANDSEED  = 26118098
DEF_TRAINP    = 0.8
DEF_VALIDATEP = 0.1
DEF_TESTP     = 0.1

# END DEFINE DEFAULTS
################################################################################
################################################################################
# BEG CLASS DEFINITIONS

class SplitDataException(Exception):
  """splitting data failed"""
  pass

# END CLASS DEFINITIONS
################################################################################
################################################################################
# BEG HELPER FUNCTIONS

def _fix_pdist(problist):
  """normalizes problist so it sums to 1.0"""
  tot = sum(problist)
  return [ x/tot for x in problist ]

def _get_seq_count(seq_groups, idx1, idx2):
  """returns number of sequences in seq_groups[idx1:idx2)"""
  count = 0
  for x in seq_groups[idx1:idx2]:
    count += x.size
  return count

def _do_write(data, outf):
  for d in data:
    outf.write(d)
    outf.write('\n')
  return

def _write_data(train, valid, test, outfilename):
  with open(outfilename, 'w') as outf:
    outf.write('#train\n')
    _do_write(train, outf)
    outf.write('#validate\n')
    _do_write(valid, outf)
    outf.write('#test\n')
    _do_write(test,  outf)
  return

def _split_data(clusterfilename, outfilename, randomseed,
                trainp, validatep, testp, verbose):
  """splits the data into (approx) trainp, validatep, testp"""

  # set up random number generator
  rng = numpy.random.default_rng(randomseed)

  if verbose:
    sys.stdout.write('random seed  {}\n'.format(randomseed))
    sys.stdout.write('cluster file {}\n'.format(clusterfilename))
    sys.stdout.write('out file     {}\n'.format(outfilename))
    sys.stdout.write('trying to split data into ')
    sys.stdout.write('{:.2f} train, '.format(round(trainp,2)))
    sys.stdout.write('{:.2f} validate '.format(round(validatep,2)))
    sys.stdout.write('and {:.2f} test.\n'.format(round(testp,2)))

  # BEG DATA READ ############################################################
  if verbose:
    sys.stdout.write('reading group sizes... ')
    sys.stdout.flush()
  # get a list of groups and membership, sorting everything to insure
  # reproducibility with the same random seed.
  # we will split the data by group, to insure that no group is split
  # across train, validate and/or test sets. This makes sure that
  # there are no 'similar' sequences across sets, where 'similar' is
  # defined as same group membership.
  sequence_groups_arr = []
  with open(clusterfilename, 'r', newline='') as handle:
    reader = csv.reader(handle, dialect='unix')
    for row in reader:
      garr = numpy.array(row)
      garr.sort()
      sequence_groups_arr.append((garr[0], garr))
  # sort groups by 'smallest' sequence id
  sequence_groups_arr.sort()
  # pack sequences into a list of numpy arrays
  sequence_groups = numpy.array([ x[1] for x in sequence_groups_arr ],
                                dtype=object)
  # calculate total sequence count
  total_seq_count = 0
  for x in sequence_groups:
    total_seq_count += x.size
  if verbose:
    sys.stdout.write('done. ')
    sys.stdout.write('found {} groups '.format(sequence_groups.size))
    sys.stdout.write('and {} sequences\n'.format(total_seq_count))
  if sequence_groups.size < 3:
    raise(SplitDataException('cant split {} groups into 3 pieces\n'.format(
                             sequence_groups.size)))
  # END DATA READ ############################################################
  # BEG GROUP SHUFFLE ########################################################
  if verbose:
    sys.stdout.write('shuffling groups... ')
    sys.stdout.flush()
  # randomize groups before split
  rng.shuffle(sequence_groups)
  if verbose:
    sys.stdout.write('done.\n')
  # END GROUP SHUFFLE ########################################################
  # BEG DATA SPLIT ###########################################################
  if verbose:
    sys.stdout.write('splitting data... ')
    sys.stdout.flush()
  # initialize split into train, validate, test data
  idx_1 = int(sequence_groups.size * trainp)
  idx_2 = int(sequence_groups.size * (trainp + validatep))
  train_n    = _get_seq_count(sequence_groups, 0,     idx_1)
  validate_n = _get_seq_count(sequence_groups, idx_1, idx_2)
  test_n     = _get_seq_count(sequence_groups, idx_2, sequence_groups.size)
  if verbose:
    sys.stdout.write('done.\n')
    # training data
    real_train_p = round(train_n/total_seq_count, 4)
    sys.stdout.write('  {:.4f} training '.format(real_train_p))
    sys.stdout.write('    ({} sequences, '.format(train_n))
    sys.stdout.write('  {} groups)\n'.format(idx_1))
    # validation data
    real_validate_p = round(validate_n/total_seq_count, 4)
    sys.stdout.write('  {:.4f} validation '.format(real_validate_p))
    sys.stdout.write('  ({}  sequences, '.format(validate_n))
    sys.stdout.write('  {}  groups)\n'.format(idx_2-idx_1))
    # testing data
    real_test_p = round(test_n/total_seq_count, 4)
    sys.stdout.write('  {:.4f} testing '.format(real_test_p))
    sys.stdout.write('     ({}  sequences, '.format(test_n))
    sys.stdout.write('  {}  groups)\n'.format(sequence_groups.size-idx_2))
  # END DATA SPLIT ###########################################################
  # BEG DATA SHUFFLE #########################################################
  if verbose:
    sys.stdout.write('shuffling each data set... ')
    sys.stdout.flush()
  # unroll and shuffle each data split
  train_data = numpy.concatenate(sequence_groups[    0:idx_1]).ravel()
  valid_data = numpy.concatenate(sequence_groups[idx_1:idx_2]).ravel()
  ttest_data = numpy.concatenate(sequence_groups[idx_2:     ]).ravel()
  rng.shuffle(train_data)
  rng.shuffle(valid_data)
  rng.shuffle(ttest_data)
  if verbose:
    sys.stdout.write('done.\n')
  # END DATA SHUFFLE #########################################################
  # BEG DATA WRITE ###########################################################
  if verbose:
    sys.stdout.write('writing data... ')
    sys.stdout.flush()
  _write_data(train_data, valid_data, ttest_data, outfilename)
  if verbose:
    sys.stdout.write('done.\n')
  # END DATA WRITE ###########################################################
  return

# END HELPER FUNCTIONS
################################################################################
################################################################################
# BEG MAIN

if __name__ == '__main__':
    # parse command-line arguments
    parser = argparse.ArgumentParser(
                      description='split PDB data into train, validate, test',
                      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    # general options
    parser.add_argument('--version', action='version', version=__version__)
    parser.add_argument('-v', '--verbose', type=distutils.util.strtobool,
                        dest='verbose',
                        help='show runtime information on stdout',
                        metavar='y|n')
    parser.add_argument('-c', '--clusterfile', dest='clusterf',
                        help='set cluster file name',
                        metavar='FILE', required=True)
    parser.add_argument('-o', '--outfile', dest='outf',
                        help='set output file name',
                        metavar='FILE', required=True)
    parser.add_argument('--rseed', type=int, dest='rseed',
                        help='set random number seed',
                        metavar='INT')
    parser.add_argument('--train', type=float, dest='trainp',
                        help='set training proportion',
                        metavar='NUM')
    parser.add_argument('--validate', type=float, dest='valp',
                        help='set validation proportion',
                        metavar='NUM')
    parser.add_argument('--test', type=float, dest='testp',
                        help='set testing proportion',
                        metavar='NUM')
    # set defaults
    parser.set_defaults(verbose=True,
                        clusterf=None,
                        outf=None,
                        rseed=DEF_RANDSEED,
                        trainp=DEF_TRAINP,
                        valp=DEF_VALIDATEP,
                        testp=DEF_TESTP)
    # parse command-line arguments
    args = parser.parse_args()

    args.trainp, args.valp, args.testp = _fix_pdist([ args.trainp,
                                                      args.valp,
                                                      args.testp ])

    # split data
    try:
        _split_data(args.clusterf, args.outf, args.rseed,
                    args.trainp, args.valp, args.testp, args.verbose)
    except Exception as e:
        sys.stderr.write('EXCEPTION: {}\n'.format(e))
        raise(e)

    if args.verbose:
        sys.stdout.write('finished.\n')

# END MAIN
################################################################################
