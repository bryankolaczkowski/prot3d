#!/usr/bin/env -S python3 -u

################################################################################
# combines sequence and evolutionary profiling data, writing tensorflow
# data records to train, validate and test directories
#
# (C) 2020 Bryan Kolaczkowski, University of Florida, Gainesville, FL USA
# Released under GNU General Public License (GPL)
# bryank@ufl.edu
################################################################################

# he he :) - get path to this running file to import the protmodel module
import os
libpath = os.path.normpath(                                                   \
            os.path.join(                                                     \
                os.path.dirname(os.path.abspath(os.path.realpath(__file__))), \
                '..')                                                         \
            )
import sys
sys.path.append(libpath)

import distutils.util
import argparse
import random
import multiprocessing
import numpy
import tensorflow
import protmodel.aafeatures
import protmodel.tfserialize

from protmodel._version import __version__

seq_db       = None
seq_db_index = None
pro_db       = None
pro_db_index = None
data_outdir  = None
aa_seq_map   = protmodel.aafeatures.AANDXred()

class WriteProcess (multiprocessing.Process):
  FILE_PREFIX    = 'd'    # prefix files with this
  FILE_EXTENSION = '.tfd' # suffix files with this

  # initialize a file writer process
  def __init__(self, proc_id, file_count, outdir, data, verbose):
    multiprocessing.Process.__init__(self)
    self.id      = proc_id
    self.data    = data
    self.nfiles  = file_count
    self.outdir  = outdir
    self.verbose = verbose
    return

  def _get_seq(self, handle, offset):
    seq = []
    handle.seek(offset)
    seqstr = handle.readline().strip()
    for c in seqstr:
      seq.append(aa_seq_map[c])
    return numpy.vstack(seq)

  def _get_pro(self, handle, offset):
    handle.seek(offset)
    handle.readline() # skip sequence id
    data = []
    line = handle.readline()
    while line and line[0] != '>':
      row = [ float(x) for x in line.strip().split(',') ]
      data.append(row)
      line = handle.readline()
    return numpy.array(data)

  def run(self):
    if self.verbose:
      sys.stdout.write('starting proc {}\n'.format(self.id))

    # set up writers
    writers = []
    for i in range(self.nfiles):
      fname = self.outdir + os.path.sep + self.FILE_PREFIX + str(self.id) + \
              '_' + str(i) + self.FILE_EXTENSION
      comp = tensorflow.io.TFRecordOptions(compression_type='ZLIB')
      writ = tensorflow.io.TFRecordWriter(fname, options=comp)
      writers.append(writ)

    # parse and write data
    with open(seq_db, 'r') as seq_file:
      with open(pro_db, 'r') as pro_file:
        writer_id = 0
        for data_id in self.data:
          # parse
          seq = self._get_seq(seq_file, seq_db_index[data_id][0])
          pro = self._get_pro(pro_file, pro_db_index[data_id])
          # serialize
          record = protmodel.tfserialize.serialize_data(seq, pro)
          # write
          writers[writer_id].write(record)
          # iterate 'round the circle of writers
          writer_id += 1
          if writer_id == len(writers):
            writer_id = 0

    # close up file writers
    for writer in writers:
      writer.close()

    if self.verbose:
      sys.stdout.write('done proc {}\n'.format(self.id))
    return



def setup_writedata(seq_id, pro_id, tvt_split, outdir, verbose):
  """sets up indexing and data for tensorflow data write"""
  global seq_db_index
  global pro_db_index

  # read sequence index
  if verbose:
    sys.stdout.write('reading sequence index...')
    sys.stdout.flush()
  seq_db_index = {}
  with open(seq_id, 'r') as handle:
    for line in handle:
      linearr = line.split()
      sid = linearr[0]
      beg = int(linearr[1])
      lgh = int(linearr[2])
      seq_db_index[sid] = (beg,lgh)
  if verbose:
    sys.stdout.write('done. found {} sequence indices\n'.format( \
                     len(seq_db_index.keys())))

  # read profile index
  if verbose:
    sys.stdout.write('reading profile index...')
    sys.stdout.flush()
  pro_db_index = {}
  with open(pro_id, 'r') as handle:
    for line in handle:
      linearr = line.strip().split(',')
      sid = linearr[0].split('_')[1]
      idx = int(linearr[1])
      pro_db_index[sid] = idx
  if verbose:
    sys.stdout.write('done. found {} profile indices\n'.format( \
                     len(pro_db_index.keys())))

  # read train-validate-test split
  if verbose:
    sys.stdout.write('reading tvt split...')
    sys.stdout.flush()
  tvt_map = {}
  with open(tvt_split, 'r') as handle:
    did  = ''
    data = []
    for line in handle:
      if line[0] == '#':
        if did and data:
          tvt_map[did] = data
        did = line[1:].strip()
        data = []
      else:
        data.append(line.strip())
    if did and data:
      tvt_map[did] = numpy.array(data)
  if verbose:
    ttl = 0
    for v in tvt_map.values():
      ttl += len(v)
    sys.stdout.write('done. found {} total data\n'.format(ttl))
    for k,v in tvt_map.items():
      p = round((len(v) / ttl) * 100, 1)
      sys.stdout.write('  {}: {} {}%\n'.format(k, len(v), p))

  return tvt_map

################################################################################
# BEG MAIN

if __name__ == '__main__':
  # parse command-line arguments
  parser = argparse.ArgumentParser(
                    description='writes tensorflow data records to disk',
                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  # general options
  parser.add_argument('--version', action='version', version=__version__)
  parser.add_argument('-v', '--verbose', type=distutils.util.strtobool,
                      dest='verbose',
                      help='show runtime information on stdout',
                      metavar='y|n')
  parser.add_argument(dest='seq_db', help='mmseqs2 sequence database',
                      metavar='DBF')
  parser.add_argument(dest='seq_id', help='mmseqs2 sequence database index',
                      metavar='DBI')
  parser.add_argument(dest='pro_db', help='evol profile database',
                      metavar='PDB')
  parser.add_argument(dest='pro_id', help='evol profile index',
                      metavar='PDI')
  parser.add_argument(dest='tvt_split', help='train-validate test split file',
                      metavar='TVT')
  parser.add_argument(dest='outdir', help='output directory',
                      metavar='DIR')
  parser.set_defaults(verbose=True,
                      seq_db=None,
                      seq_id=None,
                      pro_db=None,
                      pro_id=None,
                      tvt_split=None,
                      outdir=None)
  # parse command-line arguments
  args = parser.parse_args()

  if args.outdir[-1] == os.path.sep:
    args.outdir = args.outdir[:-1]

  if not os.path.exists(args.outdir):
    os.makedirs(args.outdir)

  for f in [args.seq_db, args.seq_id, args.pro_db, args.pro_id, args.tvt_split]:
    if not os.path.exists(f):
      sys.stderr.write('ERRR: infile {} nonexistent\n'.format(f))
      sys.exit(1)

  if args.verbose:
    sys.stdout.write('starting...\n')

  seq_db = args.seq_db
  pro_db = args.pro_db
  tvt_map = setup_writedata(args.seq_id, args.pro_id,
                            args.tvt_split, args.outdir, args.verbose)

  ## write data seems to need to be in main context, not in a function ##
  # parse train-validate-test data
  if args.verbose:
    sys.stdout.write('writing data...\n')

  # read-write data
  for datatype, data in tvt_map.items():
    if args.verbose:
      sys.stdout.write(' writing {}...\n'.format(datatype))
    # shuffle data ids
    random.shuffle(data)
    # create output directory if needed
    data_outdir = args.outdir + os.path.sep + datatype
    if not os.path.exists(data_outdir):
      os.mkdir(data_outdir)

    chunksize = int(len(data)/os.cpu_count())
    file_count = 100

    writers = []
    for i in range(os.cpu_count()):
      beg = i * chunksize
      end = (i+1) * chunksize
      if i == os.cpu_count()-1:
        end = len(data)
      mydata = data[beg:end]
      writer = WriteProcess(i, file_count, data_outdir, mydata, args.verbose)
      writers.append(writer)

    for writer in writers:
      writer.start()
    for writer in writers:
      writer.join()
    for writer in writers:
      writer.close()

    if args.verbose:
      sys.stdout.write('done.\n')

  if args.verbose:
    sys.stdout.write('finished.\n')
# END MAIN
################################################################################
